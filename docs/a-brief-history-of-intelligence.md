# 지능의 기원

::: INFO
맥스 베넷, "지능의 기원", _더퀘스트_, 김성훈 역, 2025.
:::

저자는 서문에서 1962년에 나온 SF 만화 <The Jetsons>에 등장하는 로봇 '로지'를 소개하며, 지난 수십년간 [[artificial-intelligence]] 분야가 놀라운 발전을 이룩했음에도 왜 아직 로지와 같은 로봇은 만들지 못하는지 질문한다. 이 질문에서 출발해 인류가 인간 수준의 인공지능을 만들어내는 데 얼마나 가까워졌는지 판단하기 위해 시간을 거슬러 올라가 최초의 뇌를 조사하고, 인간의 지능이 어떻게 발전했는지 재구성해본다. 이 과정에서 다섯 번의 혁신(조종, 강화, 시뮬레이션, 정신화, 언어)이 있었음을 강조한다. 저자는 계속해서 뇌의 작동 방식과 인공지능의 작동 방식을 비교하며 상호보완적으로 지능에 대한 이해를 넓혀간다.

::: INFO
저자는 경제학과 수학을 전공했고, 집필 당시에는 AI 쇼핑 에이전트를 만드는 [Alby](https://alby.com/)의 CEO였다. 
:::

## 조종

생물에게 뇌가 만들어지기 전, 단세포 생명체의 지능은 단백질의 화학 반응으로 구현되었다. 그러다가 주변의 먹이를 기다리는 대신 능동적으로 먹이를 찾아 나서는 동물이 등장했다. 이들은 환경을 탐색하고 먹이를 사냥하기 위한 방향전환 매커니즘이 필요했다. 조종(steering)에는 좌우대칭 형태가 유리했고, 외부에서 받는 감각 신호를 종합해 처리할 기관이 필요했다. 뇌가 있는 동물은 좌우대칭동물밖에 없는데, 이는 우연이 아니다.

오늘날 선충(nematode)은 좌우대칭동물의 기본 원형만 갖춘 동물이다. 선충의 행동 규칙은 두 가지로 요약할 수 있다: (1) 먹이의 냄새가 짙어지면 계속 앞으로 나아가고, (2) 냄새가 옅어지면 방향을 바꾼다. 이 단순한 조종으로 선충은 먹이를 직접 찾아나설 수 있다. 또한 선충에게는 빛, 온도, 접촉을 감지하는 감각세포가 있어서 날카로운 곳, 밝은 곳, 너무 뜨겁거나 차가운 곳에서 멀어질 수 있다. 로드니 브룩스(Rodney Brooks)를 비롯한 행동주의 [[artificial-intelligence]] 연구자들은 단순한 수준의 지능을 구현하기 전에는 절대 인간 수준의 지능을 이해할 수 없다고 주장했다. 브룩스는 생물의 운동 기능이 진화하는 데 걸린 시간에 비하면 논리 기능은 단기간에 발전했기 때문에 일단 운동 기능을 구현하면 논리 기능은 어렵지 않게 발전할 것이라고 생각했다. 브룩스가 만든 청소 로봇 룸바(Roomba)는 선충의 조종 방식과 유사했다.

조종을 위해서는 어떤 환경으로 다가가야 하는 조건(좋은 것)과 멀어져야 하는 조건(나쁜 것)을 구분할 수 있어야 한다. 선충의 머리에는 긍정적 감정가 감각신경세포와 부정적 감정가 감각신경세포가 있다. 긍정적 감정가는 먹이 냄새의 증가에 반응해 전진 신경세포에 흥분성 신호를 보낸다. 한편 부정적 감정가는 먹이 냄새의 감소, 역치 이상의 온도, 구리에 반응해 방향전환 신경세포에 흥분성 신호를 보낸다. 감정가는 동물의 내적상태(지금 배가 얼마나 고픈지)에 따라 달라진다. 전진 신경세포와 방향전환 신경세포는 모두 운동신경세포에 흥분성 신호를 보내는데, 전진 신경세포가 더 강한 신호를 보내면 앞으로 나아가고, 방향전환 신경세포가 더 강한 신호를 보내면 방향을 바꾼다. 

생물의 감정에는 두 가지 축이 있는데, 감정가와 각성이다. 이 두 속성을 정동(affect)이라고 하며, 특정 시점에 생물은 두 차원으로 표현되는 정동상태(affective state)에 놓인다. 선충이 배가 고프면 부정적 감정가와 높은 각성이 결합한 '탈출' 상태가 된다. 이때 선충은 빠르게 전진하며 먹이를 탐색한다. 먹이를 찾으면 긍정적 감정가와 높은 각성이 결합한 '활용' 상태가 되어 느리게 주변을 배회한다. 먹이를 충분히 먹으면 긍정적 감정가와 낮은 각성이 결합한 '포만' 상태가 되어 거의 움직이지 않고 휴식한다. 선충의 뇌는 신경전달물질을 이용해서 이러한 정동상태를 구현한다. 뇌는 주변에 먹이가 감지되면 도파민을, 자기 내부에서 먹이를 감지하면 세로토닌을 분비한다. 도파민은 쾌락을 느끼고 있다는 신호가 아니라 앞으로 느낄 쾌락을 예상하는 신호이며, 이 신호는 생물을 적극적으로 행동하게 만든다. 만약 선충이 스트레스를 받으면 노르에피네프린, 옥토파민, 에피네프린 등 종류가 다른 신경전달물질을 분비해 투쟁-도피 반응을 촉발한다. 이때는 수면, 번식, 소화 등 에너지 비용이 높은 행동이 억제된다. 급성 스트레스가 시작될 때 오피오이드와 같은 항스트레스 물질이 함께 분비된다. 오피오이드는 투쟁-도피 반응이 끝난 뒤 완화-회복 반응을 일으켜 폭식이나 과도한 수면을 유도한다. 이후 오피오이드 수치가 떨어지면 다시 평소 상태로 돌아온다.

선충을 일반적으로 소금물에 긍정적 감정가를 부여해 소금물 방향으로 다가간다. 그런데 소금물 속에서 배고픔을 경험한 선충들은 정반대로 행동한다. 선충이 소금물을 배고픔과 연결지어 학습한 것으로, 이를 연합학습(associative learning)이라고 한다. 연합은 새로운 경험을 통해 획득되거나 소거될 수 있다. 만약 연합이 이미 소거됐는데, 시간이 지난 뒤 다시 연합이 일어나면 처음보다 훨씬 빠른 속도로 연합을 학습하는 재획득이 일어난다. 만약 생물이 여러 자극을 동시에 받으면 어떤 자극을 연합해야 하는지 판단해야 한다. 기계학습에서는 이를 신뢰 할당 문제(credit assignment problem)라고 한다. 이 사건을 예측하기 위한 단서로 어떤 것을 신뢰할지 선택하는 것이다. 좌우대칭동물은 몇 가지 전략을 사용한다.

- 적격성 흔적(eligibility trace): 사건과 시간 간격이 짧은 단서를 선택한다.
- 가리기(overshadowing): 강도가 가장 높은 단서를 선택한다.
- 잠재적 억제(latent inhibition): 과거에 반복적으로 경험하지 않은 단서를 선택한다.
- 차폐(blocking): 이미 단서를 하나 선택했다면 추가 단서를 모두 무시한다.

## 강화

동물이 시행착오를 통해 임의의 행동 순서를 학습하는 능력을 강화학습(reinforcement learning)이라고 한다. 에드워드 손다이크(Edward Thorndike)는 긍정적 감정가는 강화하고, 부정적 감정가는 처벌하는 방식으로 동물이 학습한다고 생각했다. 그러나 마빈 민스키(Marvin Minsky)가 이 방식을 인공신경망으로 체커를 두도록 했을 때 잘 작동하지 않았다. 게임의 결과가 나왔을 때만 강화와 처벌을 할 수 있기에, 게임 내내 둔 수 중 어떤 수를 강화하고 처벌할지 결정할 수 없었기 때문이다. 리처드 서튼(Richard Sutton)은 강화학습을 행위자(actor)와 비평가(critic)라는 두 독립 요소로 분리했다. 비평가는 게임의 모든 상태에 대해 승리 확률을 평가하고, 행위자는 어떻게 행동할지 선택해 승리 확률을 높일 때마다 비평가로부터 보상을 받는다. 행위자가 학습하는 신호는 보상 자체가 아니라 어느 상태와 다음 상태 사이에 예측되는 보상의 시간적 차이이기 때문에 이를 시간차 학습(temporal difference learning)이라고 한다. 

::: NOTE
실제 뇌가 강화학습을 한다면 지도학습이 유행하는 최근 [[artificial-intelligence]] 트렌드가 정말 맞는 방향인지 다시 생각하게 된다. [[welcome-to-the-era-of-experience]]의 지적처럼 결국에는 강화학습에 대한 주목을 놓쳐서는 안 될 것 같다.
:::

시간차 학습과 실제 뇌의 매커니즘 사이에 연관성을 찾아낸 사람은 피터 다얀(Peter Dayan)이었다. 강화의 신호는 도파민이었다. 강화학습이 작동하기 위해서는 강화와 보상이 분리되어야 한다. 뇌는 실제 보상이 아니라 예측되는 미래 보상의 변화를 바탕으로 행동을 강화한다. 동물이 행위의 실제 결과가 즐겁지 않은 데도 도파민을 분비하는 행위에 중독되는 이유가 여기에 있다. 도파민 신경세포에서의 시간차 학습 신호는 선충을 비롯한 단순한 좌우대칭동물에게서 발견되지 않는다. 척추동물로 전환되는 과정에서 주변에 좋은 것이 있음을 예측하는 '원함'의 신호가 미래의 보상을 예측하는 시간차 학습 신호로 발전한 것이다. 척추동물의 도파민 분비를 조절하는 것은 시상하부다. 시상하부의 감정가 신경세포는 바닥핵(basal ganglia)의 도파민 신경세포와 연결되어 있어서 시상하부가 유쾌하면 바닥핵에 도파민을 채우고, 불쾌하면 도파민을 고갈시킨다. 시상하부가 실제 보상을 받았는지 판단하는 비평가라면 바닥핵은 운동계에 연결된 행위자인 셈이다.

::: NOTE
소셜 미디어에서 새 글을 확인하기 위해 끝없이 새로고침하는 행동이 도파민 분비 매커니즘과 관련있다는 생각을 했다. 숏폼 콘텐츠는 사실상 인간의 도파민 분비 매커니즘을 해킹한 포맷이라고 할만하다.
:::

생물은 활성화된 신경세포 하나만으로 세상에 관한 정보를 얻지 않는다. 모든 척추동물은 여러 신경세포의 패턴을 해독해서 사물을 인식한다. 여기에는 두 가지 문제가 있다: (1) 겹치는 패턴을 별개의 패턴으로 구분하는 식별 문제(discrimination problem)와 (2) 기존 패턴을 바탕으로 유사한 새 패턴을 인식하는 일반화 문제(generalization problem)다.

겉질의 피라미드 신경세포는 수천 개의 시냅스로부터 입력을 받아 패턴을 인식한다. 소수의 후각신경세포는 다수의 피라미드 신경세포에 신호를 보내며, 하나의 후각신경세포는 피라미드 신경세포의 한 부분에만 연결된다. 만약 입력 패턴이 겹치더라도 활성화된 후각신경세포로부터 입력을 받는 피라미드 신경세포가 다르다. 이 덕분에 식별 문제를 해결 할 수 있다. 또한 하나의 피라미드 신경세포는 주변의 수많은 피라미드 신경세포와 시냅스를 이룬다. 만약 어떤 냄새 패턴이 피라미드 신경세포에 특정 패턴을 활성화시키면 그 세포 조합이 자동으로 연결되어 이후 불완전한 패턴이 등장했을 때 겉질에서는 온전한 패턴을 다시 활성화할 수 있다. 이를 자동연합(auto-association)이라고 하며, 이 덕분에 겉질이 일반화 문제를 해결할 수 있다. 자동연합은 컴퓨터와 척추동물의 기억 시스템에서 중요한 차이를 보여준다. 척추동물은 컴퓨터와 달리 특정 기억을 저장하는 주소를 관리하지 않는다. 대신 경험의 부분집합을 이용해 원래의 패턴을 활성화함으로써 기억을 회상할 수 있다. 겉질이 패턴을 분리하는 능력을 갖추고 있기 때문에 새로운 패턴을 학습해도 기존 패턴을 잊지 않는다는 이론이 있다. 인공신경망은 새로운 과제를 학습하면 기존 과제를 잊어버리는 파괴적 망각 문제(problem of catastrophic forgetting) 문제를 겪는다. 파괴적 망각 문제를 회피하기 위해 대부분의 현대 [[artificial-intelligence]] 시스템은 모든 것을 한꺼번에 학습한 뒤 동결해버린다.

자동연합으로는 같은 물체를 다양한 각도에서 인식하는 불변성 문제(invariance problem)를 설명할 수 없다. 척추동물이 하나의 물체를 여러 방향에서 볼 때 입력되는 신경세포는 완전히 다르다. 그럼에도 물체를 식별할 수 있는 이유는 겉질이 계층적으로 구성되어 있기 때문이다. 시각 정보는 겉질로 전달되어 V1 부위에서 모서리와 선으로 인식되고, V2 부위에서 모양으로, V4 부위에서 물체로, IT 부위에서 정교한 특성으로 인식된다. 후쿠시마 구니히코(福島 邦彦)는 인공신경망에 입력되는 이미지를 분해해서 여러 특성 맵(feature map)을 만들었다. 맵을 통해 어떤 특성을 파악하면 그 결과를 압축해서 또 다른 특성 맵의 집합으로 전달한다. 단계가 깊어질수록 더 넓은 이미지 영역에서 더 정교한 특성을 결합할 수 있다. 이를 합성곱 신경망(convolutional neural network)이라고 한다. 합성곱 신경망이 뇌에서 영감을 받기는 했지만, 실제 뇌를 모방했다고 할 수는 없다: (1) 실제 뇌의 시각 처리는 위계구조가 엄격하지 않아서 입력이 여러 수준으로 동시에 전달되기도 하고, (2) 사물의 변환도 훨씬 잘 인식하며, (3) 지도학습과 역전파 없이 작동한다. 무엇보다 (4) 합성곱 신경망이 영감을 받은 포유류의 뇌보다 단순한 어류의 뇌도 불변성 문제를 잘 해결한다.

시간차 학습 알고리즘은 많은 비디오 게임에서 인간 플레이어의 성과를 뛰어넘었지만, 보상이 드물거나 초기 상태로부터 멀리 떨어진 게임은 해결하지 못했다. 강화학습 시스템은 활용-탐색 딜레마(exploitation-exploration dilemma)를 겪는다. 이미 알고 있는 행동을 반복(활용)하는 선택과 새로운 행동을 시도(탐색)하는 선택 사이에서 균형을 잡아야 한다는 것이다. 딥마인드(DeepMind)은 새로운 것을 탐색하는 내적 동기에 보상함으로써 문제를 해결했다. 실제로 처음 호기심을 갖게 된 생물을 척추동물이었다.

척추동물의 뇌가 지닌 또 다른 특성은 공간 과제를 해결하는 능력이다. 귀 안의 반고리관은 안뜰감각(vestibular sense)을 만들어낸다. 동물은 동일한 시각적 단서를 갖고 내가 대상을 향해 가고 있는 것인지, 대상이 나를 향해 오고 있는 것인지 판단해야 한다. 초기 척추동물의 겉질은 냄새를 인식하는 가쪽겉질, 시각과 소리 패턴을 학습하는 배쪽겉질, 그리고 안쪽겉질로 이루어져 있었다. 안쪽겉질은 포유류에서 해마가 되었다. 해마는 시각 신호와 안뜰 신호, 머리 방향 신호를 종합해 공간지도를 만든다. 뇌가 세상을 표상하는 내적 모델을 구축했다는 점이 중요하다. 초기 척추동물은 먹이가 주로 나타나는 곳, 안전한 곳, 위험한 곳을 기억하고, 목적지로 향하는 방향을 계산할 수 있었다.

## 시뮬레이션

작은 초기 포유류의 겉질 중 한 부위는 새겉질이라는 새로운 부위로 바뀌었고, 실제 행동 전에 시뮬레이션하는 능력을 얻게 되었다. 시뮬레이션이 가능하려면 멀리 내다볼 수 있는 시력과 빠른 연산을 가능하게 만든 온혈성이 필요했다. 새겉질은 시각, 청각 등 감각 정보를 입력받아 시뮬레이션으로 예측하는 데이터를 끊임없이 비교한다. 초기 척추동물이 겉질과 바닥핵으로 미래의 보상을 예측했다면, 초기 포유류는 모든 감각 정보를 종합해 미래의 상태를 예측할 수 있었다. 새겉질이 어떻게 세계 모델을 내적으로 구축하는지는 명확히 밝혀지지 않았다. 얀 르쿤(Yann Le Cun)은 현대 [[artificial-intelligence]]이 누락한 능력은 세계 모델을 구축하는 능력이며, 지능의 기질로서 언어와 기호는 과대평가 되었다고 주장했다.

새겉질은 여러 분기 중 하나를 선택하기 전에 대리 시행착오(vicarious trial and error)로 미래를 예측할 수 있고, 선택한 이후 다른 분기를 선택했다면 어땠을 지 상상하는 반사실적 학습(counterfactual learning)으로 행동을 변화시킬 수 있다. 반사실적 학습은 신뢰 할당 문제를 해결하는 데도 도움이 된다. 초기 척추동물이 미래 보상의 변화를 예상하는 시점을 바탕으로 신뢰를 할당했다면, 초기 포유류는 인과관계를 바탕으로 신뢰를 할당할 수 있다. 과거에 있었던 특정 일화를 기억하는 것도 새겉질의 시뮬레이션 기능에 의존한다. 실제로 과거의 사건을 기억할 때와 미래의 사건을 상상할 때는 비슷한 신경회로를 사용한다. 미래와 과거에 대한 시뮬레이션은 기계학습과 비슷하다. 모델 없는 강화학습(model-free reinforcement learning)은 시스템이 자극과 행동, 보상 사이에 직접적인 연합을 형성한다. 이 시스템에는 모델이 없기 때문에 결정 전에 행동을 시뮬레이션할 수 없다. 모델 기반 강화학습(model-based reinforcement learning)은 자신의 행동이 세상에 어떤 영향을 끼치는지 예측한다. 가령 딥마인드의 알파제로(AlphaZero)는 현재 게임의 상태에서 자신이 어떤 수를 두었을 때 가장 승률을 높일 수 있는지 시뮬레이션하고 다음 수를 결정한다.

포유류의 새겉질은 감각 데이터를 바탕으로 시뮬레이션하는 감각새겉질과 언제 무엇을 시뮬레이션할지 판단하는 이마엽새겉질로 나뉜다. 이마엽새겉질은 다시 운동겉질과 과립이마엽앞겉질(gPFC, granular prefrontal cortex)와 무과립이마엽앞겉질(aPFC, agranular prefrontal cortex)로 나뉜다. aPFC는 해마와 시상하부, 편도체에서 입력을 받아 동물의 행동에서 특정한 의도를 관찰해 다음 행동을 예측한다. 만약 예측과 실제가 달라 불확실성이 높아지면 aPFC는 바닥핵의 특정 부위에 일시 정지 신호를 촉발한다. 이때 동물은 가던 길을 멈추고 주변을 돌아보며 대리 시행착오를 확인한다. 알파제로가 모든 가능한 수를 살펴보는 대신 최고의 수라고 예측한 수들만 시뮬레이션하듯, aPFC도 동물이 취할 것으로 예상한 특정 경로만을 검색한다. 시뮬레이션을 바탕으로 실제 행위를 결정하는 것은 바닥핵이다. 바닥핵은 각 시뮬레이션에 따라 분비되는 도파민의 양을 바탕으로 행동을 선택한다. 바닥핵은 새겉질이 실제 세상을 시뮬레이션하는지, 상상의 세계를 시뮬레이션하는지 구분하지 못한다. aPFC가 행동을 통제하는 방식은 행동 자체를 통제하는 것이 아니라 바닥핵에 어느 선택이 더 나은지 시연하는 것이다. 바닥핵은 강화된 행동을 반복하며, 의도를 가진 것은 aPFC다. 

운동겉질은 운동을 통제하는 주요 시스템이다. 그런데 최초의 포유류는 운동겉질 없이도 정상적으로 운동할 수 있었고, 운동겉질 손상으로 마비는 영장류에게만 일어나는 현상이다. 사실 운동겉질은 aPFC와 같은 방식으로 동작하며, aPFC와 다른 점은 검색 경로에서의 운동 예측이 아닌, 특정 신체 부위의 운동 예측을 학습한다는 것이다. 운동겉질은 운동을 계획하고, 새로운 운동을 학습한다. 

목표 위계의 꼭대기에는 aPFC가 있다. aPFC는 시상하부의 활성화를 바탕으로 상위목표(검색경로)를 결정한다. 이 목표는 운동앞겉질로 전달되어 하위목표를 결정하고, 다시 운동겉질로 전달되어 특정 몸동작을 결정하는 하위-하위목표를 세운다. aPFC는 자신의 목표를 달성하는 데 필요한 특정 몸동작을 신경쓸 필요가 없다. 운동겉질은 높은 수준의 목표에 대해서 신경쓸 필요가 없다. aPFC와 운동겉질에서는 시뮬레이션이 일어난다. aPFC와 연결된 바닥핵 앞쪽 영역은 습관적 갈망을 만들어 상위 목표를 자동으로 추구하고, 바닥핵 뒤쪽 영역은 운동겉질과 연결되어 습관적인 운동 반응을 만들어낸다.

## 정신화

## 언어

## 관련문서

- [[artificial-intelligence]]
- [[welcome-to-the-era-of-experience]]

